# Atividade Pr√°tica: PySpark com Python e Docker

## Informa√ß√µes Gerais

**P√∫blico-alvo:** Alunos de gradua√ß√£o em Ci√™ncia de Dados  
**Tem√°tica:** Infraestrutura para projetos de Big Data com Apache Spark  
**N√≠vel:** Intermedi√°rio

---

## Objetivos de Aprendizagem

Ao final desta atividade, voc√™ ser√° capaz de:

1. Compreender a arquitetura e conceitos fundamentais do Apache Spark
2. Implementar transforma√ß√µes e a√ß√µes em PySpark
3. Processar dados estruturados e n√£o estruturados usando DataFrames e RDDs
4. Containerizar aplica√ß√µes Spark usando Docker
5. Aplicar opera√ß√µes de an√°lise de dados em larga escala
6. Comparar o paradigma Spark com MapReduce tradicional

---

## Pr√©-requisitos

- Conhecimento b√°sico de Python
- Familiaridade com linha de comando (terminal/bash)
- Conceitos b√°sicos de SQL (desej√°vel)
- Conta no GitHub (gratuita)
- Conta no Docker Hub (gratuita)
- Navegador web moderno

---

## Recursos Necess√°rios

Todos os recursos podem ser acessados online gratuitamente:

- **GitHub Codespaces** (ambiente de desenvolvimento na nuvem)
- **Play with Docker** (https://labs.play-with-docker.com/) - Ambiente Docker online
- **Dataset**: Dados de vendas de e-commerce para an√°lise

---

## Parte 1: Fundamentos do Apache Spark

### 1.1 O que √© Apache Spark?

Apache Spark √© um framework de processamento de dados distribu√≠do de c√≥digo aberto, projetado para ser **r√°pido**, **escal√°vel** e **f√°cil de usar**. Foi desenvolvido na UC Berkeley em 2009 e se tornou um projeto Apache em 2013.

#### Principais Caracter√≠sticas:

- **Velocidade**: At√© 100x mais r√°pido que MapReduce (processamento em mem√≥ria)
- **Facilidade de uso**: APIs em Python, Scala, Java, R e SQL
- **Generalidade**: Suporta batch, streaming, ML, grafos
- **Execu√ß√£o**: Local, cluster (Standalone, YARN, Mesos, Kubernetes)

### 1.2 Arquitetura do Spark

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SPARK APPLICATION                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              DRIVER PROGRAM                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ SparkContext/SparkSession                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Converte programa em tarefas                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Agenda tarefas nos executors                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Monitora execu√ß√£o                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                  ‚îÇ
‚îÇ                           ‚ñº                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ              CLUSTER MANAGER                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Standalone / YARN / Mesos / Kubernetes            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Gerencia recursos do cluster                      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                           ‚îÇ                                  ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ         ‚ñº                 ‚ñº                 ‚ñº               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ EXECUTOR  ‚îÇ     ‚îÇ EXECUTOR  ‚îÇ     ‚îÇ EXECUTOR  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ     ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ     ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ     ‚îÇ ‚îÇ Task  ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  Cache    ‚îÇ     ‚îÇ  Cache    ‚îÇ     ‚îÇ  Cache    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Componentes da Arquitetura:

1. **Driver Program**:
   - Executa a fun√ß√£o `main()` da aplica√ß√£o
   - Cria o SparkContext/SparkSession
   - Converte o c√≥digo em um DAG (Directed Acyclic Graph)
   - Divide o DAG em stages e tasks
   - Agenda tasks para execu√ß√£o

2. **Cluster Manager**:
   - Gerencia recursos do cluster
   - Aloca recursos para a aplica√ß√£o
   - Tipos: Standalone, YARN, Mesos, Kubernetes

3. **Executors**:
   - Processos que executam as tasks
   - Armazenam dados em cache/mem√≥ria
   - Enviam resultados ao driver

4. **Tasks**:
   - Menor unidade de trabalho
   - Enviadas aos executors
   - Processam parti√ß√µes de dados

### 1.3 Conceitos Fundamentais

#### RDD (Resilient Distributed Dataset)

RDD √© a abstra√ß√£o fundamental do Spark - uma cole√ß√£o distribu√≠da e imut√°vel de objetos.

**Caracter√≠sticas:**
- **Resiliente**: Recupera-se de falhas automaticamente
- **Distribu√≠do**: Dados particionados atrav√©s do cluster
- **Dataset**: Cole√ß√£o de dados

**Cria√ß√£o de RDD:**
```python
# A partir de uma cole√ß√£o
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

# A partir de arquivo
rdd = spark.sparkContext.textFile("data/input.txt")
```

#### DataFrames

DataFrames s√£o cole√ß√µes distribu√≠das de dados organizados em colunas nomeadas (similar a tabelas SQL ou DataFrames do pandas).

**Vantagens:**
- Otimiza√ß√£o autom√°tica (Catalyst Optimizer)
- Suporte a SQL
- Schema definido
- Melhor performance que RDDs

**Cria√ß√£o de DataFrame:**
```python
# A partir de arquivo CSV
df = spark.read.csv("data/sales.csv", header=True, inferSchema=True)

# A partir de dados em mem√≥ria
data = [(1, "Jo√£o", 1000), (2, "Maria", 1500)]
df = spark.createDataFrame(data, ["id", "nome", "salario"])
```

#### Transforma√ß√µes vs A√ß√µes

**Transforma√ß√µes** (Lazy - n√£o executam imediatamente):
- `map()`, `filter()`, `groupBy()`, `join()`, `select()`, `where()`
- Criam um novo RDD/DataFrame
- Exemplo: `df.filter(df.age > 18)`

**A√ß√µes** (Eager - disparam execu√ß√£o):
- `count()`, `collect()`, `first()`, `show()`, `save()`
- Retornam valores ao driver
- Exemplo: `df.count()`

### 1.4 Execu√ß√£o Lazy (Lazy Evaluation)

Spark utiliza **avalia√ß√£o pregui√ßosa**: transforma√ß√µes n√£o s√£o executadas imediatamente.

**Fluxo de Execu√ß√£o:**
```
C√≥digo ‚Üí DAG ‚Üí Logical Plan ‚Üí Physical Plan ‚Üí Execu√ß√£o
```

1. **DAG Creation**: Spark constr√≥i um grafo de depend√™ncias
2. **Logical Plan**: Otimiza√ß√µes l√≥gicas (Catalyst)
3. **Physical Plan**: Escolha do melhor plano f√≠sico
4. **Execution**: Tasks s√£o executadas nos executors

**Exemplo:**
```python
# Nada √© executado aqui
df_filtered = df.filter(df.age > 18)
df_selected = df_filtered.select("name", "age")

# Somente aqui a execu√ß√£o acontece
result = df_selected.count()  # A√ß√£o dispara o processamento
```

### 1.5 Spark vs MapReduce

| Caracter√≠stica | MapReduce | Spark |
|---------------|-----------|-------|
| **Velocidade** | Mais lento (disco) | At√© 100x mais r√°pido (mem√≥ria) |
| **Facilidade** | Complexo (Java) | Simples (Python, Scala, SQL) |
| **Processamento** | Batch apenas | Batch, Streaming, ML, Grafos |
| **Itera√ß√µes** | Lento (I/O disco) | R√°pido (cache em mem√≥ria) |
| **APIs** | Baixo n√≠vel | Alto n√≠vel (DataFrames, SQL) |
| **Uso** | Hadoop espec√≠fico | Multi-plataforma |

### ‚úÖ Checkpoint 1.1

Antes de prosseguir, responda:

- [ ] Voc√™ compreende a diferen√ßa entre Driver e Executor?
- [ ] Voc√™ entende o que s√£o RDDs e DataFrames?
- [ ] Voc√™ sabe a diferen√ßa entre Transforma√ß√µes e A√ß√µes?
- [ ] Voc√™ compreende o conceito de Lazy Evaluation?
- [ ] Voc√™ consegue comparar Spark com MapReduce?

---

## ‚ö†Ô∏è Importante: Permiss√µes do Docker (Windows)

### Configura√ß√µes Necess√°rias

Para garantir que os comandos Docker funcionem corretamente no Windows, siga estas etapas:

#### 1. Docker Desktop - Compartilhamento de Drive
1. Abra **Docker Desktop**
2. V√° em **Settings** ‚Üí **Resources** ‚Üí **File Sharing**
3. Certifique-se de que a unidade do projeto est√° compartilhada
4. Clique em **Apply & Restart**

#### 2. PowerShell - Permiss√µes de Execu√ß√£o
Execute no PowerShell como Administrador:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

#### 3. WSL2 (Recomendado)
```powershell
# Verificar instala√ß√£o
wsl --list --verbose

# Se necess√°rio, instalar
wsl --install
```

### Corre√ß√µes Implementadas

‚úÖ **Dockerfile**: Permiss√µes adequadas para diret√≥rios de dados  
‚úÖ **docker-compose.yml**: Configura√ß√£o `user: root` para acesso a volumes  
‚úÖ **Compatibilidade Windows**: Bind mounts funcionando corretamente

**üìÑ Para mais detalhes**, consulte: `pyspark_app/PERMISSIONS_GUIDE.md`

---

## Parte 2: Caso de Uso - An√°lise de Vendas de E-commerce

### 2.1 Contexto do Problema

**Cen√°rio**: Uma empresa de e-commerce precisa analisar suas vendas para tomar decis√µes estrat√©gicas.

**Objetivos da An√°lise**:
1. Calcular receita total por categoria de produto
2. Identificar os produtos mais vendidos
3. Analisar padr√µes de vendas por regi√£o
4. Calcular ticket m√©dio por cliente
5. Identificar tend√™ncias temporais de vendas

**Dataset**: `sales_data.csv`

**Estrutura dos dados:**
```csv
transaction_id,date,customer_id,product_id,product_name,category,quantity,price,region
TX001,2024-01-15,C101,P501,Notebook,Eletr√¥nicos,1,2500.00,Sudeste
TX002,2024-01-15,C102,P502,Mouse,Eletr√¥nicos,2,45.00,Sul
TX003,2024-01-16,C103,P503,Livro,Livros,3,35.00,Nordeste
...
```

**Campos:**
- `transaction_id`: ID √∫nico da transa√ß√£o
- `date`: Data da venda
- `customer_id`: ID do cliente
- `product_id`: ID do produto
- `product_name`: Nome do produto
- `category`: Categoria do produto
- `quantity`: Quantidade vendida
- `price`: Pre√ßo unit√°rio
- `region`: Regi√£o da venda

### 2.2 An√°lises a Realizar

#### An√°lise 1: Receita Total por Categoria
Calcular a receita total (quantidade √ó pre√ßo) agrupada por categoria.

#### An√°lise 2: Top 10 Produtos Mais Vendidos
Identificar os 10 produtos com maior volume de vendas.

#### An√°lise 3: Vendas por Regi√£o
Analisar a distribui√ß√£o de vendas entre as regi√µes do Brasil.

#### An√°lise 4: Ticket M√©dio por Cliente
Calcular o valor m√©dio gasto por cada cliente.

#### An√°lise 5: An√°lise Temporal
Identificar tend√™ncias de vendas ao longo do tempo (di√°ria/mensal).

### ‚úÖ Checkpoint 2.1

Verifique:

- [ ] Voc√™ compreende o contexto do problema de neg√≥cio?
- [ ] Voc√™ entende a estrutura dos dados?
- [ ] Voc√™ sabe quais an√°lises precisam ser realizadas?
- [ ] Voc√™ consegue pensar em como o Spark pode ajudar?

---

## Parte 3: Configura√ß√£o do Ambiente

### 3.1 Fazendo Fork e Clonando o Reposit√≥rio no GitHub Codespaces

**Passo 1:** Acesse https://github.com e fa√ßa login

**Passo 2:** Fa√ßa um fork do reposit√≥rio do laborat√≥rio

1. Acesse o reposit√≥rio original fornecido pelo professor
2. Clique no bot√£o "Fork" no canto superior direito
3. Selecione sua conta como destino do fork
4. Aguarde a cria√ß√£o do fork (alguns segundos)

**Passo 3:** Clone seu fork usando GitHub Codespaces

1. No seu fork, clique no bot√£o verde "Code"
2. Selecione a aba "Codespaces"
3. Clique em "Create codespace on main"
4. Aguarde o ambiente carregar (pode levar 2-3 minutos - Spark requer mais recursos)

**Passo 4:** **üîß IMPORTANTE - Configure as permiss√µes do Docker**

No GitHub Codespaces, √© necess√°rio adicionar seu usu√°rio ao grupo Docker:

```bash
# Execute este comando no terminal do Codespaces
sudo usermod -aG docker $USER && newgrp docker
```

**Por que isso √© necess√°rio?** O Docker daemon requer permiss√µes especiais. Este comando adiciona seu usu√°rio ao grupo `docker`, permitindo executar comandos Docker sem `sudo`.

**Passo 5:** Verifique o ambiente

```bash
python3 --version
docker --version
docker ps  # Este comando deve funcionar sem erros
```

**Se ainda houver erro de permiss√£o**, reinicie o Codespace:
- Clique nos tr√™s pontos (...) no canto superior
- Selecione "Restart Codespace"
- Aguarde reiniciar e tente novamente

**Passo 6:** Explore a estrutura do projeto

```bash
ls -la pyspark_app/
```

Voc√™ ver√°:
- `spark_sales_analysis.py` - Script principal de an√°lise
- `spark_word_count.py` - Exemplo simples (Word Count)
- `data_generator.py` - Gerador de dados de vendas
- `Dockerfile` - Configura√ß√£o do container
- `requirements.txt` - Depend√™ncias Python
- `data/` - Diret√≥rio com datasets de exemplo

### ‚úÖ Checkpoint 3.1

Verifique:

- [ ] Fork do reposit√≥rio foi criado com sucesso
- [ ] Reposit√≥rio foi clonado no Codespaces
- [ ] Python 3.x est√° instalado
- [ ] Docker est√° dispon√≠vel
- [ ] Todos os arquivos da aplica√ß√£o est√£o presentes
- [ ] Voc√™ consegue visualizar os scripts Python

---

## Parte 4: Implementa√ß√£o com PySpark

### 4.1 Explorando a Estrutura do Projeto

O reposit√≥rio j√° cont√©m todos os scripts necess√°rios. Vamos entender cada componente:

```bash
cd pyspark_app
ls -la
```

Estrutura esperada:

```
pyspark_app/
‚îú‚îÄ‚îÄ spark_sales_analysis.py    # An√°lise completa de vendas
‚îú‚îÄ‚îÄ spark_word_count.py         # Exemplo b√°sico
‚îú‚îÄ‚îÄ data_generator.py           # Gera dados de teste
‚îú‚îÄ‚îÄ spark_stream_example.py     # Exemplo de streaming
‚îú‚îÄ‚îÄ Dockerfile                  # Imagem Docker com Spark
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestra√ß√£o
‚îî‚îÄ‚îÄ data/                       # Datasets
    ‚îú‚îÄ‚îÄ sales_data.csv          # Dados de vendas
    ‚îú‚îÄ‚îÄ products.csv            # Cat√°logo de produtos
    ‚îî‚îÄ‚îÄ input.txt               # Texto para word count
```

### 4.2 Entendendo o Dataset

Primeiro, vamos gerar dados de exemplo:

```bash
python3 data_generator.py
```

Visualize o conte√∫do:

```bash
head -20 data/sales_data.csv
```

### 4.3 Exemplo Simples: Word Count com PySpark

Antes da an√°lise completa, vamos executar um exemplo simples para entender os conceitos:

```bash
cat spark_word_count.py
```

Execute o exemplo:

```bash
python3 spark_word_count.py
```

**O que este script faz:**
1. Cria uma SparkSession
2. L√™ um arquivo de texto
3. Aplica transforma√ß√µes (split, flatMap, map, reduceByKey)
4. Executa uma a√ß√£o (collect/show)

### 4.4 An√°lise de Vendas - Parte 1: Carregamento e Explora√ß√£o

Abra o arquivo `spark_sales_analysis.py` e analise o c√≥digo:

```bash
cat spark_sales_analysis.py
```

**Se√ß√£o 1: Inicializa√ß√£o**
- Cria SparkSession
- Configura mem√≥ria e cores

**Se√ß√£o 2: Carregamento de Dados**
- L√™ CSV com schema inference
- Valida dados carregados

**Se√ß√£o 3: Explora√ß√£o Inicial**
- Exibe schema
- Mostra primeiras linhas
- Calcula estat√≠sticas

### 4.5 An√°lise de Vendas - Parte 2: Transforma√ß√µes e Agrega√ß√µes

**An√°lise Implementada:**

1. **Receita por Categoria**
```python
df.withColumn("revenue", col("quantity") * col("price"))
  .groupBy("category")
  .agg(sum("revenue").alias("total_revenue"))
  .orderBy(desc("total_revenue"))
```

2. **Top Produtos**
```python
df.groupBy("product_name")
  .agg(sum("quantity").alias("total_sold"))
  .orderBy(desc("total_sold"))
  .limit(10)
```

3. **Vendas por Regi√£o**
```python
df.groupBy("region")
  .agg(
    count("*").alias("num_transactions"),
    sum(col("quantity") * col("price")).alias("total_revenue")
  )
```

### 4.6 Executando a An√°lise Completa

Execute o script de an√°lise:

```bash
python3 spark_sales_analysis.py
```

Observe a sa√≠da:
- Schema do DataFrame
- Estat√≠sticas descritivas
- Resultados de cada an√°lise
- M√©tricas de performance

### ‚úÖ Checkpoint 4.1

Verifique:

- [ ] Voc√™ conseguiu gerar os dados de vendas?
- [ ] O exemplo de Word Count executou com sucesso?
- [ ] A an√°lise de vendas foi executada completamente?
- [ ] Voc√™ compreende as transforma√ß√µes utilizadas?
- [ ] Os resultados fazem sentido do ponto de vista de neg√≥cio?

---

## Parte 5: Containeriza√ß√£o com Docker

### 5.1 Entendendo o Dockerfile

Examine o Dockerfile:

```bash
cat Dockerfile
```

**Componentes:**
- Imagem base com Spark e Python
- Instala√ß√£o de depend√™ncias
- Configura√ß√£o do ambiente Spark
- C√≥pia dos scripts

### 5.2 Construindo a Imagem Docker

**‚ö†Ô∏è Resolu√ß√£o do Erro de Permiss√£o**

Se voc√™ receber o erro:
```
ERROR: permission denied while trying to connect to the Docker daemon socket
```

**Causa**: No GitHub Codespaces, o usu√°rio precisa estar no grupo `docker` para acessar o daemon.

**Solu√ß√£o (execute uma vez)**:
```bash
# Adicione seu usu√°rio ao grupo docker
sudo usermod -aG docker $USER && newgrp docker

# Verifique se funcionou
docker ps
```

Se ainda houver problema, reinicie o Codespace (Menu ... ‚Üí Restart Codespace).

---

**Agora sim, construa a imagem:**

```bash
# Navegar para o diret√≥rio correto
cd pyspark_app

# Construir a imagem
docker build -t pyspark-app:v1.0 .
```

Aguarde o build (pode levar 3-5 minutos na primeira vez).

**Verificar a imagem criada:**
```bash
docker images | grep pyspark-app
```

### 5.3 Executando o Container

**Op√ß√£o 1: Executar an√°lise de vendas**
```bash
docker run --rm \
  -v "$(pwd)/data:/app/data" \
  pyspark-app:v1.0 \
  python spark_sales_analysis.py
```

**Op√ß√£o 2: Executar word count**
```bash
docker run --rm \
  -v "$(pwd)/data:/app/data" \
  pyspark-app:v1.0 \
  python spark_word_count.py
```

**Op√ß√£o 3: Shell interativo**
```bash
docker run -it --rm \
  -v "$(pwd)/data:/app/data" \
  pyspark-app:v1.0 \
  bash
```

### 5.4 Docker Compose

Para orquestra√ß√£o mais simples, use Docker Compose:

```bash
# An√°lise de vendas
docker-compose up sales-analysis

# Word count
docker-compose up word-count

# PySpark Shell interativo
docker-compose up pyspark-shell
```

### ‚úÖ Checkpoint 5.1

Verifique:

- [ ] A imagem Docker foi constru√≠da com sucesso?
- [ ] Os containers executam sem erros?
- [ ] Os volumes est√£o montados corretamente?
- [ ] Voc√™ consegue acessar os resultados?
- [ ] O Docker Compose est√° funcionando?

---

## Parte 6: Entreg√°veis da Atividade

### 6.1 O que deve ser entregue

Para comprovar a conclus√£o desta atividade pr√°tica, voc√™ dever√° entregar **screenshots** das execu√ß√µes na tarefa do Microsoft Teams atribu√≠da ao aluno.

### 6.2 Lista de Screenshots Obrigat√≥rios

Capture e envie os seguintes screenshots na tarefa do Teams:

**1. Fork do Reposit√≥rio**
- Screenshot mostrando seu fork do reposit√≥rio no GitHub

**2. Codespaces em Execu√ß√£o**
- Screenshot do GitHub Codespaces aberto com os arquivos do projeto

**3. Estrutura do Projeto**
- Screenshot do terminal mostrando a estrutura de arquivos com `ls -la pyspark_app/`

**4. Gera√ß√£o de Dados**
- Screenshot da execu√ß√£o do `data_generator.py` mostrando a cria√ß√£o dos arquivos CSV

**5. Execu√ß√£o do Word Count**
- Screenshot da execu√ß√£o completa do `spark_word_count.py` mostrando os resultados

**6. An√°lise de Vendas - Schema**
- Screenshot mostrando o schema do DataFrame de vendas

**7. An√°lise de Vendas - Receita por Categoria**
- Screenshot mostrando os resultados da an√°lise de receita por categoria

**8. An√°lise de Vendas - Top 10 Produtos**
- Screenshot mostrando os 10 produtos mais vendidos

**9. An√°lise de Vendas - Vendas por Regi√£o**
- Screenshot mostrando a distribui√ß√£o de vendas por regi√£o

**10. Docker Build**
- Screenshot mostrando o build da imagem Docker com sucesso

**11. Docker Images**
- Screenshot do comando `docker images` mostrando a imagem `pyspark-app:v1.0` criada

**12. Execu√ß√£o no Container**
- Screenshot mostrando a an√°lise de vendas executando dentro do container Docker

**13. Docker Compose**
- Screenshot mostrando a execu√ß√£o com `docker-compose up`

### 6.3 Orienta√ß√µes para os Screenshots

**Requisitos para os screenshots:**

1. **Qualidade**: Screenshots devem estar leg√≠veis e em resolu√ß√£o adequada
2. **Conte√∫do completo**: Capture toda a sa√≠da relevante do comando/execu√ß√£o
3. **Formato**: PNG ou JPG

**Como capturar screenshots:**
- Windows: `Windows + Shift + S`
- Mac: `Cmd + Shift + 4`
- Linux: `Print Screen` ou `Gnome Screenshot`

### 6.4 Como Entregar

1. Capture todos os 13 screenshots obrigat√≥rios
2. Nomeie os arquivos de forma descritiva (ex: `01_fork_repositorio.png`, `02_codespaces.png`, etc.)
3. Envie todos os screenshots na **tarefa do Microsoft Teams** atribu√≠da
4. Certifique-se de que todos os screenshots est√£o leg√≠veis antes de enviar

### 6.5 Checklist Pr√©-Entrega

Antes de submeter, verifique:

- [ ] Todos os 13 screenshots obrigat√≥rios foram capturados
- [ ] Screenshots est√£o leg√≠veis e mostram informa√ß√µes completas
- [ ] Arquivos est√£o nomeados de forma clara
- [ ] Todos os scripts executaram corretamente
- [ ] A imagem Docker foi constru√≠da com sucesso
- [ ] As an√°lises produziram resultados coerentes

### 6.6 D√∫vidas Frequentes

**P: Preciso publicar a imagem no Docker Hub?**  
R: N√£o √© obrigat√≥rio para esta entrega. Basta ter evid√™ncias de que construiu e executou localmente.

**P: O que fazer se meu Codespaces expirar?**  
R: Voc√™ pode recriar o Codespace do seu fork. Os arquivos estar√£o l√° se voc√™ fez commit.

**P: Posso trabalhar localmente ao inv√©s de usar Codespaces?**  
R: Sim, desde que consiga executar todas as partes e gerar as evid√™ncias.

---

## Parte 7: Recursos Adicionais e Pr√≥ximos Passos

### 7.1 Conceitos Avan√ßados para Estudo

1. **Spark SQL**: Queries SQL em DataFrames
2. **Spark Streaming**: Processamento de dados em tempo real
3. **Spark MLlib**: Machine Learning distribu√≠do
4. **GraphX**: Processamento de grafos
5. **Delta Lake**: ACID transactions em Data Lakes

### 7.2 Recursos de Aprendizagem

**Documenta√ß√£o Oficial**:
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)

**Cursos Online**:
- [Databricks Academy](https://www.databricks.com/learn/training)
- [Coursera - Big Data Specialization](https://www.coursera.org/specializations/big-data)

**Livros**:
- "Learning Spark" (O'Reilly)
- "Spark: The Definitive Guide" (O'Reilly)

**Comunidade**:
- [Stack Overflow - Apache Spark](https://stackoverflow.com/questions/tagged/apache-spark)
- [Spark User Mailing List](https://spark.apache.org/community.html)

### 7.3 Pr√≥ximos Passos

1. **Deploy em Cluster**: Configure Spark em modo cluster (Standalone/YARN)
2. **Integra√ß√£o com Cloud**: Use AWS EMR, Azure Databricks ou GCP Dataproc
3. **Streaming**: Implemente processamento em tempo real com Kafka
4. **Machine Learning**: Crie modelos preditivos com MLlib
5. **Otimiza√ß√£o**: Aprenda t√©cnicas de tuning e particionamento

### 7.4 Certifica√ß√µes

- **Databricks Certified Associate Developer for Apache Spark**
- **Cloudera Certified Spark and Hadoop Developer**

---

## Ap√™ndice A: Troubleshooting

### üö® Problema: "Permission denied" ao acessar Docker daemon (GITHUB CODESPACES)

Este √© o problema **mais comum** ao executar o item 5.2 do roteiro no GitHub Codespaces.

**Erro completo**:
```
ERROR: permission denied while trying to connect to the Docker daemon socket at 
unix:///var/run/docker.sock: Head "http://%2Fvar%2Frun%2Fdocker.sock/_ping": 
dial unix /var/run/docker.sock: connect: permission denied
```

**Causa**: O usu√°rio n√£o tem permiss√µes para acessar o Docker daemon.

**‚úÖ Solu√ß√£o R√°pida** (execute no terminal do Codespaces):
```bash
sudo usermod -aG docker $USER && newgrp docker
```

**Teste se funcionou**:
```bash
docker ps
```

**Se ainda n√£o funcionar**:
1. Clique nos tr√™s pontos `...` no canto superior do Codespaces
2. Selecione **"Restart Codespace"**
3. Aguarde reiniciar e teste novamente: `docker ps`

**Solu√ß√£o Alternativa** - Use o script autom√°tico:
```bash
chmod +x setup-docker-permissions.sh
bash setup-docker-permissions.sh
```

**üìÑ Para mais detalhes**, consulte: `CODESPACES_SETUP.md`

---

### Problema: "Java not found"

**Solu√ß√£o**:
```bash
# No Codespaces
sudo apt-get update
sudo apt-get install -y default-jdk
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### Problema: "Out of Memory"

**Solu√ß√£o**:
```python
spark = SparkSession.builder \
    .appName("SalesAnalysis") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()
```

### Problema: "Permission Denied"

**Solu√ß√£o**:
```bash
chmod +x *.py
```

### Problema: Docker Build Falha

**Solu√ß√£o**:
```bash
# Limpe cache do Docker
docker system prune -a

# Rebuild sem cache
docker build --no-cache -t pyspark-app:v1.0 .
```

---

## Ap√™ndice B: Comandos √öteis

### PySpark Shell Interativo

```bash
# Inicie o PySpark shell
pyspark

# Com configura√ß√µes personalizadas
pyspark --master local[4] --driver-memory 2g
```

### Spark Submit

```bash
# Submeta uma aplica√ß√£o
spark-submit \
  --master local[*] \
  --driver-memory 2g \
  --executor-memory 2g \
  spark_sales_analysis.py
```

### Monitoramento

```bash
# Spark UI (quando executando localmente)
# Acesse: http://localhost:4040
```

---

## Conclus√£o

Parab√©ns! Voc√™ completou o laborat√≥rio de PySpark. 

**O que voc√™ aprendeu**:
- ‚úÖ Arquitetura e conceitos do Apache Spark
- ‚úÖ Diferen√ßa entre RDDs e DataFrames
- ‚úÖ Transforma√ß√µes e A√ß√µes
- ‚úÖ Lazy Evaluation
- ‚úÖ An√°lise de dados com PySpark
- ‚úÖ Containeriza√ß√£o de aplica√ß√µes Spark
- ‚úÖ Compara√ß√£o com MapReduce

**Pr√≥ximos passos**:
- Continue praticando com datasets reais
- Explore Spark Streaming e MLlib
- Considere certifica√ß√µes
- Contribua com projetos open source

---

**Desenvolvido para o curso de Ci√™ncia de Dados**  
**Vers√£o 1.0 - Novembro 2025**

**Autor**: Professor/Instrutor  
**Contato**: professor@cienciadados.edu  
**Licen√ßa**: MIT - Livre para uso educacional

# PySpark Big Data Application

Aplica√ß√£o de an√°lise de dados usando Apache Spark e PySpark, containerizada com Docker para an√°lises de Big Data em e-commerce.

## üöÄ Quick Start

### Pr√©-requisitos
- Python 3.8+
- Apache Spark 3.5+ (para execu√ß√£o local)
- Docker 20.10+ (opcional, para containeriza√ß√£o)
- Docker Compose 2.0+ (opcional, para orquestra√ß√£o)
- Java 11+ (necess√°rio para Spark)

### Executar Localmente

```bash
# Navegue at√© o diret√≥rio da aplica√ß√£o
cd pyspark_app

# Gere os dados de exemplo
python3 data_generator.py

# Execute a an√°lise de vendas
python3 spark_sales_analysis.py

# Ou execute o exemplo de word count
python3 spark_word_count.py
```

### Executar com Docker

```bash
# Construa a imagem
docker build -t pyspark-app:v1.0 .

# Gere os dados
docker run --rm -v "$(pwd)/data:/app/data" pyspark-app:v1.0 python3 data_generator.py

# Execute an√°lise de vendas
docker run --rm -v "$(pwd)/data:/app/data" pyspark-app:v1.0 python3 spark_sales_analysis.py
```

### Executar com Docker Compose

```bash
# Gerar dados
docker-compose --profile setup up data-generator

# An√°lise de vendas (perfil padr√£o)
docker-compose up sales-analysis

# Word count
docker-compose --profile examples up word-count

# PySpark Shell interativo
docker-compose --profile interactive up pyspark-shell
```

## üìä Estrutura do Projeto

```
pyspark_app/
‚îú‚îÄ‚îÄ spark_sales_analysis.py    # An√°lise completa de vendas de e-commerce
‚îú‚îÄ‚îÄ spark_word_count.py         # Exemplo b√°sico de word count
‚îú‚îÄ‚îÄ data_generator.py           # Gerador de dados sint√©ticos
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ Dockerfile                  # Configura√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestra√ß√£o de containers
‚îú‚îÄ‚îÄ .dockerignore               # Arquivos ignorados pelo Docker
‚îî‚îÄ‚îÄ data/                       # Datasets
    ‚îú‚îÄ‚îÄ sales_data.csv          # Dados de vendas (gerado)
    ‚îú‚îÄ‚îÄ products.csv            # Cat√°logo de produtos (gerado)
    ‚îú‚îÄ‚îÄ input.txt               # Texto para word count (gerado)
    ‚îî‚îÄ‚îÄ output/                 # Resultados das an√°lises
```

## üéØ Caso de Uso: An√°lise de Vendas de E-commerce

### Contexto
Sistema de Business Intelligence para an√°lise de vendas de uma empresa de e-commerce, processando transa√ß√µes para gerar insights de neg√≥cio.

### An√°lises Implementadas

1. **Receita por Categoria**: Identifica√ß√£o das categorias mais lucrativas
2. **Top Produtos**: Ranking dos produtos mais vendidos
3. **Vendas por Regi√£o**: Distribui√ß√£o geogr√°fica das vendas
4. **M√©tricas de Clientes**: Ticket m√©dio e segmenta√ß√£o de clientes
5. **Tend√™ncias Temporais**: Padr√µes de vendas ao longo do tempo
6. **Performance de Produtos**: An√°lise detalhada por produto

### Dados Utilizados

**Dataset**: `sales_data.csv`

**Schema**:
- `transaction_id`: ID √∫nico da transa√ß√£o
- `date`: Data da venda
- `customer_id`: ID do cliente
- `product_id`: ID do produto
- `product_name`: Nome do produto
- `category`: Categoria (Electronics, Books, Stationery, Accessories)
- `quantity`: Quantidade vendida
- `price`: Pre√ßo unit√°rio
- `region`: Regi√£o (Southeast, South, Northeast, North, Midwest)

## üß™ Exemplos de Uso

### Word Count B√°sico

Demonstra conceitos fundamentais do Spark:
- RDDs (Resilient Distributed Datasets)
- DataFrames
- Spark SQL
- Transforma√ß√µes e A√ß√µes
- Lazy Evaluation

```bash
python3 spark_word_count.py
```

### An√°lise de Vendas Completa

Demonstra an√°lises complexas de dados:
- Agrega√ß√µes m√∫ltiplas
- Joins e filtros
- Window functions
- SQL queries
- Cache e otimiza√ß√£o

```bash
python3 spark_sales_analysis.py
```

## üìö Conceitos PySpark Demonstrados

### Arquitetura Spark
- **Driver Program**: Coordena a execu√ß√£o
- **Cluster Manager**: Gerencia recursos
- **Executors**: Processam tarefas
- **Tasks**: Menor unidade de trabalho

### RDDs vs DataFrames
- **RDD**: Abstra√ß√£o de baixo n√≠vel, API funcional
- **DataFrame**: Alto n√≠vel, otimiza√ß√µes autom√°ticas (Catalyst)

### Transforma√ß√µes (Lazy)
```python
df.filter(col("price") > 100)
df.groupBy("category").agg(sum("revenue"))
df.select("customer_id", "total_spent")
```

### A√ß√µes (Eager)
```python
df.count()
df.show()
df.collect()
df.write.csv("output.csv")
```

### Otimiza√ß√µes
- **Catalyst Optimizer**: Otimiza√ß√£o de queries
- **Tungsten Engine**: Gerenciamento de mem√≥ria
- **Adaptive Query Execution**: Ajustes din√¢micos

## üîß Configura√ß√£o

### Configura√ß√µes do Spark

```python
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()
```

### Vari√°veis de Ambiente

```bash
export SPARK_HOME=/opt/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_PYTHON=python3
```

## üìà Resultados

Os resultados das an√°lises s√£o salvos em:
- `data/output/revenue_by_category/`: Receita por categoria
- `data/output/top_products/`: Top produtos
- `data/output/sales_by_region/`: Vendas por regi√£o
- `data/output/customer_metrics/`: M√©tricas de clientes
- `data/output/monthly_trends/`: Tend√™ncias mensais
- `data/output/product_performance/`: Performance de produtos

## üéì Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: An√°lise de Sazonalidade
Identifique padr√µes de vendas por dia da semana.

**Dica**: Use `dayofweek()` e `dayofmonth()`.

### Exerc√≠cio 2: Clientes VIP
Liste os top 20 clientes que mais gastaram.

**Dica**: Use `groupBy()` + `agg()` + `orderBy()`.

### Exerc√≠cio 3: Produtos Frequentemente Comprados Juntos
An√°lise de cesta de compras.

**Dica**: Use `self-join` em transactions do mesmo cliente.

### Exerc√≠cio 4: An√°lise de Crescimento
Compare vendas m√™s a m√™s.

**Dica**: Use Window functions com `lag()`.

### Exerc√≠cio 5: Otimiza√ß√£o de Performance
Compare tempos de execu√ß√£o com diferentes configura√ß√µes.

```bash
# Teste diferentes n√∫meros de parti√ß√µes
spark-submit --conf spark.sql.shuffle.partitions=4 spark_sales_analysis.py
spark-submit --conf spark.sql.shuffle.partitions=8 spark_sales_analysis.py
spark-submit --conf spark.sql.shuffle.partitions=16 spark_sales_analysis.py
```

## üÜö Compara√ß√£o: MapReduce vs Spark

| Aspecto | MapReduce | Spark |
|---------|-----------|-------|
| **Velocidade** | Baseline | 10-100x mais r√°pido |
| **API** | Complexa (Java) | Simples (Python, SQL) |
| **Processamento** | Disco | Mem√≥ria |
| **Itera√ß√µes** | Lento | R√°pido (cache) |
| **Casos de Uso** | Batch simples | Batch, Streaming, ML |
| **Curva de Aprendizado** | √çngreme | Suave |

## üêõ Troubleshooting

### Java not found
```bash
# Instale OpenJDK 11
sudo apt-get install openjdk-11-jdk

# Configure JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### Out of Memory
```python
# Aumente mem√≥ria
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()
```

### Permission Denied
```bash
chmod +x *.py
```

### Data not found
```bash
# Gere os dados primeiro
python3 data_generator.py
```

## üìñ Recursos de Aprendizagem

### Documenta√ß√£o Oficial
- [Apache Spark](https://spark.apache.org/docs/latest/)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### Livros Recomendados
- "Learning Spark" (O'Reilly)
- "Spark: The Definitive Guide" (O'Reilly)
- "High Performance Spark" (O'Reilly)

### Cursos Online
- [Databricks Academy](https://www.databricks.com/learn)
- [Coursera - Big Data with Spark](https://www.coursera.org/)
- [edX - Spark Fundamentals](https://www.edx.org/)

## üöÄ Pr√≥ximos Passos

1. **Spark Streaming**: Processamento em tempo real
2. **MLlib**: Machine Learning distribu√≠do
3. **GraphX**: An√°lise de grafos
4. **Delta Lake**: ACID transactions
5. **Cloud Deployment**: AWS EMR, Azure Databricks, GCP Dataproc



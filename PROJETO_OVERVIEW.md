# PySparkContainer - LaboratÃ³rio de Big Data

## ğŸ“š Sobre o Projeto

Este repositÃ³rio contÃ©m um roteiro prÃ¡tico completo para aprendizado de **Apache Spark** e **PySpark**, desenvolvido para estudantes de CiÃªncia de Dados. Ã‰ uma continuaÃ§Ã£o natural do laboratÃ³rio de MapReduce, oferecendo uma visÃ£o moderna de processamento de Big Data.

## ğŸ¯ Objetivos Principais

- Compreender a arquitetura distribuÃ­da do Apache Spark
- Dominar conceitos de RDDs, DataFrames e Spark SQL
- Implementar anÃ¡lises de dados em larga escala
- Containerizar aplicaÃ§Ãµes Spark com Docker
- Comparar paradigmas MapReduce vs Spark
- Aplicar conhecimentos em casos de uso reais

## ğŸ“ Estrutura do RepositÃ³rio

```
PySparkContainer/
â”œâ”€â”€ README.md                          # Roteiro principal (teoria + prÃ¡tica)
â”œâ”€â”€ ENTREGA_TEMPLATE.md               # Template para relatÃ³rio do aluno
â”œâ”€â”€ ORIENTACOES_PROFESSOR.md          # Guia de correÃ§Ã£o para o professor
â”œâ”€â”€ COMPARACAO_MAPREDUCE_SPARK.md     # AnÃ¡lise comparativa detalhada
â”œâ”€â”€ EXERCICIOS_EXTRAS.md              # 10 exercÃ­cios adicionais
â”œâ”€â”€ PROJETO_OVERVIEW.md               # Este arquivo (visÃ£o geral)
â”œâ”€â”€ LICENSE                           # LicenÃ§a MIT
â”œâ”€â”€ init-repo.sh                      # Script de setup (Linux/Mac)
â”œâ”€â”€ init-repo.ps1                     # Script de setup (Windows)
â”œâ”€â”€ .gitignore                        # Arquivos ignorados pelo Git
â”œâ”€â”€ .devcontainer/                    # ConfiguraÃ§Ã£o GitHub Codespaces
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ evidencias/                       # Pasta para screenshots dos alunos
â”‚   â””â”€â”€ README.md                     # InstruÃ§Ãµes sobre evidÃªncias
â””â”€â”€ pyspark_app/                      # AplicaÃ§Ã£o PySpark
    â”œâ”€â”€ README.md                     # DocumentaÃ§Ã£o da aplicaÃ§Ã£o
    â”œâ”€â”€ requirements.txt              # DependÃªncias Python
    â”œâ”€â”€ Dockerfile                    # Imagem Docker
    â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o
    â”œâ”€â”€ .dockerignore                 # Arquivos ignorados pelo Docker
    â”œâ”€â”€ data_generator.py             # Gerador de dados sintÃ©ticos
    â”œâ”€â”€ spark_word_count.py           # Exemplo bÃ¡sico (RDD + DataFrame)
    â”œâ”€â”€ spark_sales_analysis.py       # AnÃ¡lise completa de vendas
    â”œâ”€â”€ spark_stream_example.py       # Exemplo de streaming
    â””â”€â”€ data/                         # Datasets
        â”œâ”€â”€ .gitkeep
        â”œâ”€â”€ sales_data.csv            # Dados de vendas (gerado)
        â”œâ”€â”€ products.csv              # CatÃ¡logo (gerado)
        â”œâ”€â”€ input.txt                 # Texto para word count (gerado)
        â””â”€â”€ output/                   # Resultados das anÃ¡lises
```

## ğŸš€ Quick Start

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/seu-usuario/PySparkContainer.git
cd PySparkContainer
```

### 2. Execute o Setup (GitHub Codespaces recomendado)

**Linux/Mac:**
```bash
chmod +x init-repo.sh
./init-repo.sh
```

**Windows (PowerShell):**
```powershell
.\init-repo.ps1
```

### 3. Execute os Exemplos

```bash
cd pyspark_app

# Gera dados (se ainda nÃ£o executou o setup)
python3 data_generator.py

# Exemplo bÃ¡sico: Word Count
python3 spark_word_count.py

# AnÃ¡lise completa: Vendas
python3 spark_sales_analysis.py
```

### 4. Docker (Opcional)

```bash
# Build da imagem
docker build -t pyspark-app:v1.0 .

# ExecuÃ§Ã£o
docker-compose up sales-analysis
```

## ğŸ“– ConteÃºdo do Roteiro

### Parte 1: Fundamentos do Apache Spark
- Arquitetura (Driver, Executors, Cluster Manager)
- RDDs vs DataFrames
- TransformaÃ§Ãµes e AÃ§Ãµes
- Lazy Evaluation
- ComparaÃ§Ã£o com MapReduce

### Parte 2: Caso de Uso - E-commerce
- Contexto: AnÃ¡lise de vendas online
- Dataset: TransaÃ§Ãµes com mÃºltiplas dimensÃµes
- 6 anÃ¡lises implementadas:
  1. Receita por categoria
  2. Top 10 produtos
  3. Vendas por regiÃ£o
  4. MÃ©tricas de clientes
  5. TendÃªncias temporais
  6. Performance de produtos

### Parte 3: ConfiguraÃ§Ã£o do Ambiente
- GitHub Codespaces
- InstalaÃ§Ã£o de dependÃªncias
- ConfiguraÃ§Ã£o do Spark

### Parte 4: ImplementaÃ§Ã£o com PySpark
- Scripts Python comentados
- GeraÃ§Ã£o de dados
- AnÃ¡lise de vendas completa
- MÃ©tricas e resultados

### Parte 5: ContainerizaÃ§Ã£o com Docker
- Dockerfile otimizado
- Docker Compose
- ExecuÃ§Ã£o em container
- Volumes e persistÃªncia

### Parte 6: EntregÃ¡veis da Atividade
- Template de relatÃ³rio (ENTREGA.md)
- 13 screenshots obrigatÃ³rios
- CritÃ©rios de avaliaÃ§Ã£o
- Checklist de conclusÃ£o
- OrientaÃ§Ãµes de entrega

### Parte 7: Recursos Adicionais
- Conceitos avanÃ§ados para estudo
- Links para documentaÃ§Ã£o
- PrÃ³ximos passos
- CertificaÃ§Ãµes

### Parte 8: Checklist Final
- VerificaÃ§Ã£o de cÃ³digo
- VerificaÃ§Ã£o de Docker
- VerificaÃ§Ã£o de documentaÃ§Ã£o
- OrientaÃ§Ãµes para entrega

## ğŸ“ Caso de Uso: AnÃ¡lise de Vendas

### Dataset: `sales_data.csv`

**Campos:**
- `transaction_id`: ID Ãºnico
- `date`: Data da venda
- `customer_id`: ID do cliente
- `product_id`: ID do produto
- `product_name`: Nome do produto
- `category`: Categoria (Electronics, Books, etc.)
- `quantity`: Quantidade
- `price`: PreÃ§o unitÃ¡rio
- `region`: RegiÃ£o brasileira

**AnÃ¡lises:**
- Receita total por categoria
- Produtos mais vendidos
- DistribuiÃ§Ã£o geogrÃ¡fica
- SegmentaÃ§Ã£o de clientes (VIP, Premium, Regular)
- PadrÃµes temporais (mensal, semanal)

## ğŸ†š ComparaÃ§Ã£o com MapReduce

| Aspecto | MapReduce | Spark |
|---------|-----------|-------|
| **Velocidade** | Baseline | 10-100x |
| **CÃ³digo** | ~40 linhas | ~15 linhas |
| **APIs** | Baixo nÃ­vel | Alto nÃ­vel |
| **Use Cases** | Batch | Batch + Streaming + ML |
| **Curva de Aprendizado** | Ãngreme | Moderada |

**RecomendaÃ§Ã£o:** Use Spark para novos projetos!

## ğŸ’¡ Conceitos PySpark Demonstrados

### BÃ¡sicos
- âœ… SparkSession e SparkContext
- âœ… RDDs (Resilient Distributed Datasets)
- âœ… DataFrames e SQL
- âœ… TransformaÃ§Ãµes (map, filter, groupBy)
- âœ… AÃ§Ãµes (count, collect, show)

### IntermediÃ¡rios
- âœ… AgregaÃ§Ãµes complexas (sum, avg, count)
- âœ… Window functions
- âœ… Joins e unions
- âœ… Cache e persist
- âœ… Particionamento

### AvanÃ§ados
- âœ… Catalyst Optimizer
- âœ… Lazy Evaluation
- âœ… DAG (Directed Acyclic Graph)
- âœ… Physical Plans
- âœ… Adaptive Query Execution

## ğŸ³ Docker

### Imagens DisponÃ­veis

- **Base**: `apache/spark-py:v3.5.0`
- **Custom**: Adiciona scripts e dependÃªncias

### ServiÃ§os Docker Compose

```yaml
services:
  data-generator    # Gera dados sintÃ©ticos
  word-count        # Exemplo bÃ¡sico
  sales-analysis    # AnÃ¡lise completa (padrÃ£o)
  pyspark-shell     # Shell interativo
  jupyter           # Jupyter Notebook (opcional)
```

### Comandos Ãšteis

```bash
# Build
docker-compose build

# Executar anÃ¡lise
docker-compose up sales-analysis

# Shell interativo
docker-compose --profile interactive up pyspark-shell

# Jupyter Notebook
docker-compose --profile jupyter up jupyter
```

## ğŸ“Š Resultados

ApÃ³s executar as anÃ¡lises, encontre os resultados em:

```
pyspark_app/data/output/
â”œâ”€â”€ revenue_by_category/
â”œâ”€â”€ top_products/
â”œâ”€â”€ sales_by_region/
â”œâ”€â”€ customer_metrics/
â”œâ”€â”€ monthly_trends/
â””â”€â”€ product_performance/
```

## ğŸ¯ ExercÃ­cios Extras

10 exercÃ­cios adicionais disponÃ­veis em `EXERCICIOS_EXTRAS.md`:

1. **Cohort Analysis** - AnÃ¡lise de retenÃ§Ã£o
2. **Anomaly Detection** - DetecÃ§Ã£o de outliers
3. **RFM Analysis** - SegmentaÃ§Ã£o de clientes
4. **Market Basket** - AnÃ¡lise de associaÃ§Ã£o
5. **Time Series** - SÃ©ries temporais
6. **Geospatial** - AnÃ¡lise geogrÃ¡fica
7. **CLV** - Customer Lifetime Value
8. **Performance** - OtimizaÃ§Ã£o de queries
9. **Join Optimization** - Tipos de join
10. **Data Quality** - ValidaÃ§Ã£o de dados

## ğŸ“š Recursos de Aprendizado

### DocumentaÃ§Ã£o Oficial
- [Apache Spark Docs](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### Livros Recomendados
- "Learning Spark" (O'Reilly)
- "Spark: The Definitive Guide" (O'Reilly)
- "High Performance Spark" (O'Reilly)

### Cursos Online
- [Databricks Academy](https://www.databricks.com/learn)
- [Coursera - Big Data Specialization](https://www.coursera.org/)
- [edX - Spark Fundamentals](https://www.edx.org/)

### CertificaÃ§Ãµes
- Databricks Certified Associate Developer
- Cloudera Spark and Hadoop Developer

## ğŸš€ PrÃ³ximos Passos

ApÃ³s completar este laboratÃ³rio:

1. **Spark Streaming** - Processamento em tempo real
2. **MLlib** - Machine Learning distribuÃ­do
3. **GraphX** - AnÃ¡lise de grafos
4. **Delta Lake** - ACID transactions
5. **Cloud Deployment** - AWS EMR, Azure Databricks, GCP Dataproc

## ğŸ¤ Contribuindo

Este Ã© um projeto educacional. ContribuiÃ§Ãµes sÃ£o bem-vindas:

1. Fork o repositÃ³rio
2. Crie uma branch: `git checkout -b feature/nova-analise`
3. Commit: `git commit -m 'Adiciona nova anÃ¡lise'`
4. Push: `git push origin feature/nova-analise`
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

MIT License - Veja [LICENSE](LICENSE) para detalhes.

Livre para uso educacional e comercial.

## ğŸ‘¨â€ğŸ« CrÃ©ditos

**Desenvolvido para:**
- FATEC - Faculdade de Tecnologia
- Curso: CiÃªncia de Dados
- Disciplina: Infraestrutura para CiÃªncia de Dados

**VersÃ£o:** 1.0  
**Data:** Novembro 2025  
**Autor:** Professor/Instrutor

## ğŸ› Troubleshooting

### Java nÃ£o encontrado
```bash
sudo apt-get install openjdk-11-jdk
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### Out of Memory
```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```

### Arquivos nÃ£o encontrados
```bash
python3 data_generator.py
```

### Permission Denied
```bash
chmod +x *.py *.sh
```

## ğŸ“§ Suporte

- **Issues**: Use o GitHub Issues
- **DiscussÃµes**: GitHub Discussions
- **Email**: professor@fatec.edu

## â­ Reconhecimentos

Agradecimentos especiais a:
- Apache Spark Community
- Databricks por recursos educacionais
- Alunos que contribuÃ­ram com feedback

---

## ğŸ“ˆ EstatÃ­sticas do Projeto

- **Linhas de cÃ³digo:** ~2.000+
- **Scripts Python:** 4 principais
- **ExercÃ­cios:** 10 extras
- **Tempo estimado:** 8-10 horas
- **NÃ­vel:** IntermediÃ¡rio

---

**ğŸ“ Bons estudos e bom aprendizado com PySpark!**

Se este repositÃ³rio foi Ãºtil, considere dar uma â­!

---

**Links RÃ¡pidos:**
- [ğŸ“– Roteiro Principal](README.md)
- [ğŸ†š ComparaÃ§Ã£o MapReduce vs Spark](COMPARACAO_MAPREDUCE_SPARK.md)
- [ğŸ’ª ExercÃ­cios Extras](EXERCICIOS_EXTRAS.md)
- [ğŸ“¦ AplicaÃ§Ã£o PySpark](pyspark_app/README.md)
